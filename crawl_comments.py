from googleapiclient.discovery import build
import pandas as pd
from tqdm import tqdm
import time
import os
from datetime import datetime

# ğŸ”‘ API key
API_KEY = "AIzaSyB1AJ862OVBHBDIZSASiEgCHzHo1_ramiE"

# âš™ï¸ File output cá»‘ Ä‘á»‹nh
OUTPUT_FILE = "comments.csv"

# âš™ï¸ Khá»Ÿi táº¡o YouTube API
youtube = build("youtube", "v3", developerKey=API_KEY)

# ğŸ“Œ Láº¥y danh sÃ¡ch 50 video trending VN
def get_trending_videos(max_results=50, region="VN"):
    request = youtube.videos().list(
        part="snippet",
        chart="mostPopular",
        regionCode=region,
        maxResults=max_results
    )
    response = request.execute()
    videos = []
    for item in response["items"]:
        videos.append({
            "videoId": item["id"],
            "title": item["snippet"]["title"],
            "category": item["snippet"]["categoryId"]
        })
    return videos

# ğŸ“Œ Láº¥y toÃ n bá»™ comments cá»§a 1 video
def get_all_comments(video_id, title, category):
    comments = []
    next_page_token = None
    while True:
        try:
            request = youtube.commentThreads().list(
                part="snippet",
                videoId=video_id,
                maxResults=100,
                pageToken=next_page_token,
                textFormat="plainText"
            )
            response = request.execute()
            for item in response["items"]:
                top = item["snippet"]["topLevelComment"]["snippet"]
                comments.append({
                    "videoId": video_id,
                    "title": title,
                    "category": category,
                    "comment": top["textDisplay"],
                    "commentLikes": top["likeCount"],
                    "crawl_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                })
            next_page_token = response.get("nextPageToken")
            if not next_page_token:
                break
        except Exception as e:
            print(f"âš ï¸ Video {video_id} bá»‹ lá»—i hoáº·c táº¯t bÃ¬nh luáº­n: {e}")
            break
        time.sleep(0.2)
    return comments

# ğŸš€ Báº¯t Ä‘áº§u crawl
print("ğŸ“¥ Äang láº¥y danh sÃ¡ch 50 video trending táº¡i Viá»‡t Nam...")
videos = get_trending_videos()

all_comments = []
for video in tqdm(videos, desc="ğŸ“„ Crawling comments"):
    all_comments.extend(get_all_comments(video["videoId"], video["title"], video["category"]))

df = pd.DataFrame(all_comments)

# ğŸ’¾ Append vÃ o file comments.csv (náº¿u Ä‘Ã£ cÃ³)
if os.path.exists(OUTPUT_FILE):
    df.to_csv(OUTPUT_FILE, mode="a", index=False, header=False, encoding="utf-8-sig")
else:
    df.to_csv(OUTPUT_FILE, index=False, encoding="utf-8-sig")

print(f"\nâœ… Crawl xong! ÄÃ£ thÃªm {len(df)} comments vÃ o {OUTPUT_FILE}")
