import os
import pandas as pd
from googleapiclient.discovery import build

# ==== C·∫•u h√¨nh ====
API_KEY = "AIzaSyB1AJ862OVBHBDIZSASiEgCHzHo1_ramiE"
INPUT_FILE = "youtube_trending.csv"    # file video trending (c√≥ c·ªôt region)
OUTPUT_FILE = "comments.csv"           # n∆°i l∆∞u comment
CRAWLED_VIDEOS_FILE = "crawled_videos.csv"  # l∆∞u danh s√°ch video ƒë√£ crawl

# ==== ƒê·ªçc danh s√°ch video, ch·ªâ l·∫•y region = VN ====
df = pd.read_csv(INPUT_FILE)

if "region" in df.columns:
    df = df[df["region"] == "VN"]

if "videoId" in df.columns:
    video_ids = df["videoId"].dropna().astype(str).unique().tolist()
elif "url" in df.columns:
    df["videoId"] = df["url"].str.extract(r"v=([\w-]{11})")
    video_ids = df["videoId"].dropna().astype(str).unique().tolist()
else:
    raise ValueError("Kh√¥ng t√¨m th·∫•y c·ªôt videoId ho·∫∑c url trong file csv")

print(f"üì∫ T·ªïng s·ªë video Vi·ªát Nam trong d·ªØ li·ªáu: {len(video_ids)}")

# ==== Load danh s√°ch video ƒë√£ crawl ====
if os.path.exists(CRAWLED_VIDEOS_FILE):
    crawled_df = pd.read_csv(CRAWLED_VIDEOS_FILE)
    crawled_videos = set(crawled_df["videoId"].astype(str))
else:
    crawled_videos = set()

print(f"‚úÖ ƒê√£ crawl {len(crawled_videos)} video tr∆∞·ªõc ƒë√≥")

# ==== Build YouTube API ====
youtube = build("youtube", "v3", developerKey=API_KEY)

# ==== L·∫•y mapping categoryId -> categoryName ====
def get_category_mapping(region="VN"):
    cats = {}
    req = youtube.videoCategories().list(part="snippet", regionCode=region)
    res = req.execute()
    for item in res["items"]:
        cats[item["id"]] = item["snippet"]["title"]
    return cats

category_map = get_category_mapping()

# ==== H√†m l·∫•y title + category c·ªßa nhi·ªÅu video (batch 50 id) ====
def get_video_infos(video_ids):
    infos = {}
    for i in range(0, len(video_ids), 50):
        batch = video_ids[i:i+50]
        try:
            req = youtube.videos().list(part="snippet", id=",".join(batch))
            res = req.execute()
            for item in res.get("items", []):
                vid = item["id"]
                snippet = item["snippet"]
                infos[vid] = {
                    "title": snippet.get("title"),
                    "category": category_map.get(snippet.get("categoryId"), "Unknown")
                }
        except Exception as e:
            print(f"‚ùå L·ªói khi l·∫•y info batch {batch}: {e}")
    return infos

# ==== H√†m crawl comment ====
def get_comments(video_id, max_comments=100):
    comments = []
    try:
        req = youtube.commentThreads().list(
            part="snippet",
            videoId=video_id,
            maxResults=100,
            textFormat="plainText"
        )
        while req and len(comments) < max_comments:
            res = req.execute()
            for item in res.get("items", []):
                top = item["snippet"]["topLevelComment"]["snippet"]
                comments.append({
                    "comment": top.get("textDisplay"),
                    "commentLikes": top.get("likeCount", 0)
                })
                if len(comments) >= max_comments:
                    break
            req = youtube.commentThreads().list_next(req, res)
    except Exception as e:
        if "quotaExceeded" in str(e):
            print("‚ùå H·∫øt quota, d·ª´ng crawl!")
            return None
        elif "commentsDisabled" in str(e):
            print(f"üö´ Video {video_id} t·∫Øt comment.")
            return []
        else:
            print(f"‚ö†Ô∏è L·ªói khi crawl comment video {video_id}: {e}")
            return []
    return comments

# ==== Crawl to√†n b·ªô video ====
all_data = []
crawled_list = []

# L·∫•y th√¥ng tin t·∫•t c·∫£ video tr∆∞·ªõc (title, category)
video_infos = get_video_infos(video_ids)

for vid in video_ids:
    if vid in crawled_videos:
        print(f"‚û°Ô∏è B·ªè qua {vid} (ƒë√£ crawl)")
        continue

    if vid not in video_infos:
        print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y th√¥ng tin video {vid}")
        continue

    print(f"üöÄ Crawl video {vid} ...")
    title = video_infos[vid]["title"]
    category = video_infos[vid]["category"]

    comments = get_comments(vid, max_comments=100)
    if comments is None:  # quota exceeded
        break
    if len(comments) == 0:
        print(f"‚ö†Ô∏è Video {vid} kh√¥ng c√≥ comment")
        continue

    for c in comments:
        all_data.append({
            "videoId": vid,
            "title": title,
            "category": category,
            "comment": c["comment"],
            "commentLikes": c["commentLikes"]
        })

    crawled_list.append(vid)

    # L∆∞u t·∫°m comment (append th√™m)
    if all_data:
        pd.DataFrame(all_data).to_csv(
            OUTPUT_FILE,
            mode="a",
            header=not os.path.exists(OUTPUT_FILE),
            index=False,
            encoding="utf-8-sig"
        )
        all_data = []  # reset ƒë·ªÉ kh√¥ng b·ªã ghi tr√πng

    # L∆∞u t·∫°m danh s√°ch video ƒë√£ crawl
    pd.DataFrame({"videoId": list(crawled_videos | set(crawled_list))}).to_csv(
        CRAWLED_VIDEOS_FILE, index=False, encoding="utf-8-sig"
    )

print("üéâ Crawl xong! K·∫øt qu·∫£ ƒë√£ l∆∞u trong comments.csv")
